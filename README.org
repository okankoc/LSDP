;; This buffer is for notes you don't want to save, and for Lisp
;; evaluation.  If you want to create a file, visit that file with C-x
;; C-f, then enter the text in that file's own buffer.

#-*- mode: org -*-
#+STARTUP: showall
#+TODO: WAITING TODO IN-PROGRESS DONE
#+TAGS: { @journal(j) @conference(c) @idea(i) @general(g) }

* Learning To Serve 						   :@journal:
** Reading/Writing
*** Reading
**** TODO Check out new papers on deep RL for robotics 
Sergey Levine and Pieter Abbeel's new papers come to mind.
Especially: TRPO, Guided Policy Search, ...
***** TODO Read Latent Space Policy Search
Kevin Down et al. Compare with Bayes Opt approaches.
***** TODO Read Massive Uninformed Search vs RL
Benjamin Recht et al.

*** Writing
**** DONE Better plot for the Elastic Net path
Include in the paper an Elastic Net path using LARS that makes more sense.

**** TODO Add an example of a learned ranking
with basis function parameters that illustrate the importance of the learned
movement primitive.
**** DONE More discussion of Alg2 optimization procedure
The algorithm selects very different parameters (even the number!) depending
on the initial starting widths, heights, etc.
**** TODO Plot sample pruning path
Include plot of how the algorithms remove some of the variables, i.e.
87-80-70-70-... on average
**** WAITING Third journal draft
***** DONE Read all reviewers comments
***** DONE Prepare rebuttal
Added responses to all the reviewers' comments
Added a summary of the major points as bullet points
***** IN-PROGRESS Revise the paper
****** DONE Revise the intro section
******* DONE Why don't we learn in Cartesian/task space?
I argue that inverse kinematics would be an artifically introduced
burden on top of the original (difficult) problem
******* DONE Clarify that demonstrations are using kinesthetic teach-in
******* DONE Modify Figure 1 and add cuts of demonstrations
Figure 1 should have a clear background, also including cuts from
demonstrations is a good idea.
****** DONE Revise the related work section 
******* DONE Revise Lasso and Elastic Net motivation
******* DONE Motivate/compare more to previous MP methods
e.g. DMPs, ProMPs
****** IN-PROGRESS Revise method section
******* DONE Remake Figure 2
I zoomed in on a new training plot with 20 demonstrations
Here the emergence of the new parameters was more visible
******* IN-PROGRESS Clarify the use of ranking the parameters (subsection C)
******** TODO Add maybe plots of the ranked features?
******* IN-PROGRESS Discuss computational complexity 
******** TODO Check python code for derivatives
Complexity of grad. eval + residuals (as function eval.)
Include in appendix?
******** IN-PROGRESS Check python code for speed
BFGS seems to be taking longer than the multi-Task elastic Net?
********* IN-PROGRESS Experiments based on parameter size would be enlightening
Averaging would let me guess the comp. time
******** IN-PROGRESS Investigate Multi-task case
In the normal Lasso, time-complexity is O(N*p^2). I suspect 
it should be multiplied by $d$, the degree
******* DONE Compare the proposed methods LSDP and cLSDP
in terms of computational complexity, compare also to DMPs
****** IN-PROGRESS Revise experiments section with new robot experiments
******* DONE Discuss the joints in Figure 3
In caption as well as in the script
******* TODO Include smoothened ball positions in Figure 4
******* IN-PROGRESS Discuss table in more detail
******** DONE Explain the columns
******** TODO Demonstrate why sparsity is important
******** TODO Explain cLSDP fitting with more/less demonstrations
In particular, how does having higher residuals relate to the
quality of the solution? 
******* DONE Discuss the 'quality' of the demonstrations, 
Are they important for the actual execution also?
******* DONE Discuss the generalization abilities of the method
What are the limitations of the approach?
The number of demonstrations would I think be the main bottleneck.
***** IN-PROGRESS Run the python code again
***** DONE Look into C++ serve code
***** DONE Remake the videos
****** DONE Check all the previous recordings
****** DONE Clean the cluttered background
****** DONE Add the demonstration recordings
****** DONE Reshoot the robot video from different postures
****** DONE Compare with DMPs
****** DONE Cut the videos and create one video
****** WAITING RL experiments to illustrate effects of sparsity?
***** TODO Read paper one more time before submitting
***** DONE Add this to the table tennis serve branch
** Theory
*** WAITING Learn invariants of motion/policy from demonstrations
The idea is to learn invariants of motion / parameterize the relative
motion between the joints using a graph (graphical model?).

The improvement for RL is to find a reduced dim. latent space on which
we can perform policy search/BO.

**** WAITING Learn a policy that depends on joint state and ball state
The policy that the graph generates is relative, i.e. it can be
grounded by specifying an initial joint state and possibly also ball
state.

Ideally the policy should also have low accelerations and ideally it
should involve some sort of planning, e.g. it should know
intrinsically the value of a certain distance from the ball.

**** WAITING Learn a policy that understands the intention of the teacher
Inverse Reinforcement Learning maybe? To learn the Value Function
of the teacher?

*** TODO Can we learn a model of the ball takeoff?
Possibly from movement primitives and/or RL trajectories one could
learn such a model.
*** TODO Can the traj generation be solved with polynomials?
Does the full planning problem with the two ball takeoff and racket
impact models included result in a continuous trajectory? hybrid?
polynomials?
*** TODO Check local perturbative feedback solutions
If we can additionally solve for local feedback, then one does not
need to optimize repeatedly 3rd/5th order 
*** TODO Reinforcement Learning
**** WAITING How to apply model-free RL
**** TODO How to apply model-based RL
The approach must clearly be model-based as I do not want to have 'N'
rollouts for each policy $\theta_k$.

As opposed to GPs or neural networks, we could first try local linear
models as an extension of DMPs.
***** TODO Learn model of what?
Model for the ball take-off, ball-racket impact or robot dynamics? Or
all three? Couple the ball as part of the state as well?
***** TODO Is the ball or the robot rest posture a context?
We have to make sure that local policy generalizes to different ball
positions and robot rest postures.

**** IN-PROGRESS How to apply BO
In high dimensional cases it seems important to do dimensionality
reduction.

The initial posture of the robot corresponds to the 'context' of the
problem and contextual bandits would learn a GP over the contexts as
well.
***** DONE How to do dimensionality reduction on policy params
Jan suggested performing PCA analysis first but in the end 
I proposed (Lasso-like) multi-task Elastic Net coupled with feature
adaptation.
***** TODO Check out latent space approaches to BO
The recent BO survey should be of help.
***** TODO Read the recent gradient based BO paper in NIPS last year
***** TODO Investigate hierarchical approaches to Bayes Optimization
UCT algorithm comes to mind

** Simulations
*** DONE Add unit test with optimization
New serve() function in SL can perform an optimization whenever the
movement is predicted to fail. The movement will then switch the
optimization if a feasible solution is found.

- Added optim, detach flags
- Input q_act is parameter to initialize the optimization
- Rate of optim and other flags from player.cfg are replicated in
  serve.cfg 
- Desired land location flag also imported from player.cfg

*** DONE Calculate derivatives of movement primitive cost function
Check the python code for the derivatives
**** DONE Test derivatives
Added pytest in Python to check for derivatives
**** DONE Add Elastic Net path after LSDP 
Generated Elastic Net path after training (coupled) LSDP parameters
*** TODO Policy Search review
We have rl-experiments/ repository where I include
the REINFORCE algorithm that I emplemented

The MIPS MATLAB Policy Search and iLQG MATLAB code is also 
included.

**** IN-PROGRESS Check RL in MATLAB
***** TODO Test Power in MATLAB
using Jens Kober's implementation
****** TODO Adapt to my own DMP implementation
****** TODO Add an internal system
e.g. a Barrett WAM forward dynamics
****** TODO Increase dimensions of the state
***** TODO Add Reps in MATLAB
***** IN-PROGRESS Check Simone's RL toolbox (mips)
***** TODO Test against a realistic mismatch scenario
****** TODO Compare against ILC?
******* TODO Extend ILC with improved trajectories
As we use ILC and update the models, the models can also be used to
update the reference trajectories progressively.
******** TODO Relation to DDP?
**** TODO Check reinforce for convergence
REINFORCE is not converging!
***** TODO Test gradients
***** TODO Test calc_log_der 
Calculates the log derivatives 
***** TODO Test baseline subtraction
***** TODO Test against LQR solution (as baseline?)
***** TODO What are the effects of the hyperparameters?
Learning rate, policy structure, number of basis functions,
obs. noise, etc.
***** TODO Implement adaptive learning rates for reinforce
**** TODO Extend library with other policy search algorithms
**** WAITING Add interface to MUJOCO
**** TODO Read up on deep reinforcement learning blog post
** Experiments
*** DONE Hold the ball on the racket with an egg-holder
Heiko 3d-print-ed an egg holder for me to try
*** DONE Learning from Demonstrations
I'm trying at the moment to record ball positions as well during the
recordings.

The demonstrations are now shown in the new BORIS SL
(sebastian/sl_xeno/sl_wam)
with mass = 0.375 and mcm = 0.0! 
These are not optimal parameters at all! However after the racket
holder change, this was the best setting without the PD control that I
found. 

**** DONE Can we teach the robot how to serve
With the egg-holder it is possible to teach, but the shoulder is as
suspected difficult to move, even with the PD turned off.  I was
moving more the elbow so far.
**** DONE Can we learn/extract movement primitives?
***** DONE Record movement in gravity compensation mode
PD should be switched off.
***** DONE Extract movement primitives
Extracting movement primitives with 5 examples so far.  One can
remove/add more examples and also basis functions to test on the
robot.
***** DONE Test on robot
Testing on the robot was tricky, as we need to make sure that during
the kinesthetic teachin, the joint limits are never exceeded.  It
seems that the hardware limits are not the same as the software
limits.

After teaching for the second time with the eggholder on the red side
of the racket, the movement could be executed on the robot.

**** DONE Get ball positions during the teaching
*** DONE Tune the DMPs
We should have a json file with additional settings that we can
adjust.
**** DONE Pretty format the JSON file
MATLAB dumps the JSON file in an unreadable way.  Opening in Python
and formatting solves the problem.
**** DONE Adjust timing of the motion
Unit test added that checks if subsampling the sped-up signal restores
it to the original movement.

Silly mistake found where qd_des, qdd_des were not updated!
**** DONE Train on different examples in the dataset
Having multiple json files could easily solve this problem.
**** DONE Start on different initial conditions
Testing different postures, different DMPs and different starting
positions we see that the I.C changes the accelerations like crazy!
**** DONE Resolve the problem of high initial accelerations
Jens idea to reduce the initial accelerations does not work
unfortunately. Instead we can penalize the accelerations using Radial
Basis Functions.
***** DONE Penalize the accelerations throughout
This actually generates smoothing splines in the nonparametric case
and in the parametric case when we also put an l1 penalty on the
parameters we get a (multitask) Elastic Net!

*** IN-PROGRESS Test the sparse RBFs on real robot
Testing the learned sparse RBFs on real robot is next.
**** IN-PROGRESS Figure out the bug 
There seems to be a bug in the C++ code.
*** IN-PROGRESS Test ball information
**** DONE Test ZMQ interface during human demonstrations
***** DONE Copy new trained vision parameters to json files
The new logistic regression parameters must all be negative.
I saved the projection matrices also in a serve_3d_ping_okan file. The
matrices are not normalized (i.e., P[3,4] is not 1) but triangulation
in c++ does not cause a problem.
***** DONE Add 2d interface to listener
Instead of running an external triangulation server, I can do the
triangulation in a detached thread inside the table tennis library.
****** DONE Add triangulation method
In Python I use SVD to solve for the rightmost singular vector, this
is a linear triangulation method.
***** DONE Test online with demonstrations 
It is easy to debug the interface, I can stream the received 2d pixels
and the triangulated 3d positions to a debug file online
**** IN-PROGRESS Questions about ball interface
***** TODO How to use time stamp?
I use now the num variable received from the vision server
***** TODO Why so many files depend on ball_interface?
If the header changes almost all the library recompiles
***** TODO Should saved proj. matrices be normalized?
So far triangulation did not pose a problem
***** TODO How to invert projection matrices in test
The linear triangulation method with SVD does not work in test.
Instead least squares with P0[1:2,:] and P1[1:2,:] rows concatenated
matrix gives the exact inverse in test.
***** DONE No need to send zeros if ball is not detected
This will obviously pose a problem online. Instead now I update the
ball positions only if triangulation succeeds.
**** TODO Test ZMQ interface during robot serve
**** IN-PROGRESS Plot ball and robot data during serve
Checking the ball data in python, we observe that the balls disappear
during the motion if the movement starts in the back, should be closer
to the cameras.

The zeros received in the ball positions are removed and the balls are 
plotted via the timing with the joints. [We get absolute time both
from joint positions and ball positions saved]
***** TODO How does the ball filtering look like?
***** DONE Does serving closer to the table help detection?
Yes it does!
**** TODO Listen2D flag is buggy after resetting (pressing 'f')
*** TODO Test optimization with movement primitives
Even if the ball-takeoff model is not known, one can use movement
primitives for at least the first part of the movement.
**** DONE When to optimize?
I think when it is clear that the racket will not return the separated
ball one can 'fire' the optimization.
**** TODO Compare third order to fifth order on robot
Third order or fifth order (minimum jerk)? Which is better?
***** TODO Is it necessary to do MPC?
I would guess fifth order should be better in this case
*** IN-PROGRESS Use Reinforcement Learning to improve execution
The Desiderata for RL in robotics:
1. Data efficiency
- no rollouts
- this implies model based or something in between (BayesOpt.?)
2. Safety
- small and/or local perturbations
- model based is more promising
3. Reward engineering
- unsupervised learning?
- reward shaping?
- building models on trajectories
4. Parameterized policies
- starting overparameterized or underparam.
- relation to ILC as a result of optim.

**** TODO What is the reward function to use?
The possibilities are:

1. Minimum distance between the ball and the racket during movement
2. Minimum distance between the ball and the robot court center
3. Maximum y-velocity of the ball

or possibly a combination of the above.
***** TODO Can we adapt the reward function?
Reward shaping comes to mind here.

